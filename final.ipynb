{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGSpLjgFzqmz"
   },
   "source": [
    "<h1 style=\"text-align: center\"> MSDM5002 Project: Five-in-a-Row Game </h1>\n",
    "\n",
    "\n",
    "Group membersÔºö\n",
    "- Shihang Qiu: 20757855\n",
    "- Zhao Yuheng: 20744444\n",
    "- Deng Kaifang: 20744808\n",
    "- Lyu Lanlan: 20754736\n",
    "- Jiang Xingzhe: 20756394"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6_3XbZJ1IIq"
   },
   "source": [
    "# 1. Group Essay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woex0so6ASqa"
   },
   "source": [
    "## 1.1 Monte Carlo Tree Search\n",
    "\n",
    "Monte Carlo Tree Search is a heuristic search algorithm for kinds of decision processes.It combines the generality of random simulation with the accuracy of tree search.\n",
    "\n",
    "The rapid attention of MCTS is mainly due to the success of the computer Go program and its potential application to many difficult problems. Beyond the game itself, MCTS can theoretically be used in any field where {state state, action} is used to define and use simulation to predict output results.\n",
    "\n",
    "The algorithm process can be divided into four main steps: Selection, Expansion, Simulation and Backpropagation.The search tree would repeat the process many times and take the highest reward action as its decision. And in this project it can be described as the action with the highest winning probabiliby in each move.\n",
    "\n",
    "More detail would be introduced in the following contents.\n",
    "\n",
    "\n",
    "![](https://chiuchiu.oss-cn-shenzhen.aliyuncs.com/img/20201220170136.jpg)\n",
    "\n",
    "### 1.1.1 Selection \n",
    "\n",
    "In this project, my duty is to complete the Selection part of MCTS.\n",
    "This is the first thing we need to do to start the MCTS. In my opinion, it should be split into two parts.\n",
    "\n",
    "At the very early time in MCTS, we have to select these unvisited nodes at first. Make sure every child nodes of the root node has been simulated and backpropagated once.\n",
    "\n",
    "Next, We need to choose the most worthwhile nodes for further expansion and simulation. We can determine the expansion value of the node through the ‚ÄúUpper Confidence Bound Apply to Tree (UCT)‚Äù algorithm:\n",
    "\n",
    "$$UCT(v_i)=\\frac{Q(v_i)}{N(v_i)}+c*\\sqrt{\\frac{ln(N(v))}{N(v_i)}}$$\n",
    "\n",
    "In this equation,$Q\\left(v_i\\right)$ is the Number of simulated winning results of node $v_i$ and $N\\left(v_i\\right)$ is the Number of node $v_i$‚Äôs simulations. $\\frac{{Q}\\left({v}_{i}\\right)}{{N}\\left({v}_{i}\\right)}$ , also called ‚Äúexploitation component‚Äù, represents the traversal value of the node itself, which means expanding this node can better judge whether it is an appropriate decision or not; $\\sqrt{\\frac{{lnN}\\left({v}\\right)}{{N}\\left({v}_{i}\\right)}}$ is called ‚Äúexploration component‚Äù, represents the tree's willingness to explore less frequently visited nodes. This part can be adjusted to a proper state by the coefficient ${c}$.\n",
    "\n",
    "The selection process is: Starting from the root node, find the node with the largest UCT among the child nodes. If this node is not a leaf node, repeat the above steps from the current node until the leaf node with the largest UCT is found. If this node is already in the game over state, return the current node; Otherwise, expand the current node.\n",
    "\n",
    "\n",
    "### 1.1.2 Expansion:\n",
    "\n",
    "After we get the node that needs to be expanded, we need to find all the child nodes that represent the next step or situation of the game according to the rules of the game and randomly select a child node that has not been simulated.\n",
    "\n",
    "As the node has not been visited, we set it's $Q(v_i)=0$ and $N(v_i)=0$ and put it into the next step: simulation.\n",
    "\n",
    "### 1.1.3 Simulation:\n",
    "\n",
    "Now we already choose a node to perform the simulation.Starting from the current node we choose, we let the computer take a sequence of moves and ends the simulation in the terminal node where we can confirm the game result.\n",
    "\n",
    "In generating the sequence of moves,we can choose the legal action in the current state randomly, but we can also use some method to choose action in each move.And the end of the simulation, the simulation result will be saved in the current node.\n",
    "\n",
    "### 1.1.4 Backpropagation\n",
    "\n",
    "After simulating in the current node, we need to perform Backpropagation.\n",
    "\n",
    "In this step, we should propagate back the simulation result saved in the current node. Starting from the current node, it will find its parent node and update the statistics information of its parent node, which process will be repeated until the root node has been updated.\n",
    "\n",
    "In details, the statistics information of a node can be abstracted as the total simulation reward $Q(v)$ and the total number of visits $N(v)$.\n",
    "### 1.1.5 Advantages of MCTS\n",
    "MCTS provides a better method than traditional tree search.\n",
    "\n",
    "AheuristicÔºöMCTS does not require any strategy or specific practical knowledge about a given domain to make a reasonable decision. This algorithm can work effectively without any knowledge of game games other than the basic rules; this means that a simple MCTS implementation can be reused in many game games with only minor adjustments, so this also makes MCTS is a good method for general gaming games.\n",
    "\n",
    "AsymmetricÔºöMCTS performs an asymmetric tree adaptation to the growth of search space topology. This algorithm will visit more interesting nodes more frequently and focus its search time on more relevant parts of the tree.\n",
    "\n",
    "Asymmetric growthÔºöThis makes MCTS more suitable for game games with larger branching factors, such as 19X19 Go. Such a large combination space will cause problems for standard depth or width-based search methods, so the adaptability of MCTS shows that it can (eventually) find those more optimized actions and focus the search work on these parts.\n",
    "\n",
    "Any timeÔºöThe algorithm can terminate at any time and return the most current estimate. The currently constructed search tree can be discarded or reused later.\n",
    "\n",
    "ConciseÔºöAlgorithm implementation is very convenient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM7ufkL58gSS"
   },
   "source": [
    "## 1.2 Reinforcement Learning\n",
    "\n",
    "Reinforcement learning here is mainly composed of two parts, one is environment, the other is Policy (Figure1). The environment consists of three parts (state $x_k$, action $u_k$, and reward $g_k$). Informally, the environment is a function of black-box whose output is the current state and the reward of the previous action, and the input is the action. In this case, the position of the pieces on the current board is the state, and if we choose a move, the state of the board will change, and the good or bad move we choose before is our reward. A policy is a function that abstracts into an input state and outputs an action. Policy is similar to the thinking process of human beings. The chess player (Policy) makes an action by observing the state. Therefore, reinforcement learning can be understood as finding a function of the input state output action to maximize the reward of our environmental feedback. (The flow chart of the  algorithm is as below.)\n",
    "\n",
    "![](https://chiuchiu.oss-cn-shenzhen.aliyuncs.com/img/20201220150247.png)\n",
    "\n",
    "If the size of the gobang board is 8 x 8, there are (8*8)! Possibilities. And win or lose only at the end of a game of chess can be judged, before this it is difficult for us to measure a move good or bad. This requires Monte Carlo Tree Search algorithm. In this part, we mainly introduce monte Carlo search combined with neural network. \n",
    "\n",
    "For the neural network part, we used the alpha Zero training model. Since we did not have enough time for large-scale training, we combined the trained model with our Monte Carlo tree on the basis of understanding the model principle. And then we let them play each other. As a result, AI can be greatly improved in speed and accuracy, especially in 8*8 board. Then we explain our understanding and application of AlphaZero.\n",
    "\n",
    "First of all, the difference in MCTS is mainly reflected in the different information stored in the tree structure, and the calculation formula of UCT is also slightly different. Finally, after MCTS search is completed, AlphaGo Zero also has its own strategy to select the real placement point.\n",
    "\n",
    "In our training process, the model played a move each time, there will be two processes. One process is simulation, and one is play out. The simulation process can be understood as prediction before we actually walk, and the actual play out can be made according to our prediction results.\n",
    "\n",
    "### 1.2.1.simulationÔºö\n",
    "Different from traditional Monte Carlo tree search, our simulation is guided by neural network output results. The MCTS we use here are are four steps (Figure2).\n",
    "\n",
    "![](https://chiuchiu.oss-cn-shenzhen.aliyuncs.com/img/20201220150352.png)\n",
    "\n",
    "Firstly, in the selection stage, we select the node under the guidance of the neural network and select the node with the maximum upper confidence bound value each time. Here UCB is a formula that balances confidence (or variance) and exploration values:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UCB &= Q(s,a) + U(s,a) \\\\\n",
    "U(s,a) &\\sim c \\times P(s,a) + \\frac{\\sqrt{\\sum_b N(s,b)}}{1+N(s,a)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In short, both unexplored actions and actions that have been explored many times but have high exploration feedback will have a higher UCB value. $Q$ represents the value to be explored in the Monte Carlo tree, and the update of $Q$ is the average value of each backpropagation feedback value in the previous Monte Carlo tree search. While the $P$ value is the prior probability, which is the value calculated by the neural network, while $N$ is the exploration times of this action, and $c$ is a hyperparameter used to adjust the exploration of the equilibrium model. Therefore, the characteristic of this formula is that an action with a good reward for repeated exploration will have a high value, as well as an action without exploration. That is, why we have a well balance between exploration and optimality.\n",
    "\n",
    "Secondly, in the expand and evaluate stage, it starts with an empty search tree, expanding one node (state) at a time. When a new node is encountered, the rollout is not performed, but the new prediction node is retrieved from the neural network model to obtain the positions $s_L$ of each possible child node of the current leaf node, the drop probability $p_a$  and the corresponding value $v$ÔºàIt is evaluated by neural rather than a random explorationÔºâ. For these possible new nodes, we create them in MCTS and initialize the information saved on its branch as follows:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "N(S_L,a)=0 \\\\\n",
    "W(S_L,a)=0 \\\\\n",
    "Q(S_L,a)=0 \\\\\n",
    "P(S_L,a)=p_a \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Finally, in stage of backpropagation, we need to do the backtracking and add the information of the new leaf node branch to the ancestor node branch. The back of the logic is very simple, from each leaf node ùêø in turn back to the root node, and update the data in the upper branch structure in turn.\n",
    "\n",
    "### 1.2.2. Play\n",
    "\n",
    "When the simulation process is over, we enter the sub-process. In this step, we use the ratio between the exploration times of different actions as our actual probability distribution, and we select our actions according to this probability distribution. The probability distribution here can be used when training neural networks.\n",
    "\n",
    "### 1.2.3. Conclusion\n",
    "\n",
    "In this part, we applied MCTS search tree together with neural network to optimize neural network parameters through MCTS search tree, which in turn guides MCTS search through optimized neural network. One is the main and one is the auxiliary, which solves the chess problem with such a fully visible and well-informed state. Its algorithms allow machines to learn to do everything in a universal way like humans and to take into account the impact on the environment in a way that traditional machine learning can't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJs0a_W21N9O"
   },
   "source": [
    "# 2. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_hCI8uw1PQa"
   },
   "source": [
    "common import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-7r13dfp5Uwk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.0 (SDL 2.0.12, python 3.8.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-n07wP134kF"
   },
   "source": [
    "### 2.1 How to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhlzniX33nOO"
   },
   "source": [
    "**run all the blocks below first**, then run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4JJHKRB3oLL"
   },
   "outputs": [],
   "source": [
    "gomoku = Gomoku(size=6, player1=HumanPlayer(), player2=MCTSPlayer(max_time=10), gui=True)\n",
    "gomoku.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUVKmx_A59wk"
   },
   "source": [
    "- `size` specify the height and the width of the board\n",
    "- `player` could either be\n",
    "  - `HumanPlayer()` or `MCTSPlayer(max_time=10)` (`max_time` specify the maximum \"thinking\" time of computer)\n",
    "- set `gui=True` if you want a graphical user interface, this should be set to true if there is a `HumanPlayer()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WV3IWPMJNYH"
   },
   "source": [
    "## 2.2 MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3SDtfyweJMao"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from GomokuGameState import GomokuGameState\n",
    "import copy\n",
    "\n",
    "class MonteCarloTreeNode:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, parent, prior_prob=None):\n",
    "        \"\"\"\n",
    "        initilization for Monte Carlo Tree Node\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        state: (n, n) numpy array\n",
    "        parent: parent tree node\n",
    "        @ËµµÂÆáÊÅí\n",
    "        \"\"\"\n",
    "\n",
    "        self.child = dict() # action:childnode\n",
    "        self.parent = parent\n",
    "\n",
    "        self.is_visted = False\n",
    "\n",
    "        self._untried_actions = None\n",
    "\n",
    "        self.prior_prob = prior_prob\n",
    "\n",
    "\n",
    "        # node's statistics\n",
    "        self.n_win = 0\n",
    "        self.n_lose = 0\n",
    "        self.n_visit = 0\n",
    "\n",
    "    def expand(self, state):\n",
    "        \"\"\"\n",
    "        expand current node\n",
    "\n",
    "        Parameters:\n",
    "        --------\n",
    "        state: GomokuGameState\n",
    "            the corresponding game state of the node\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        action: (int, int)\n",
    "            next action\n",
    "        child_node: MonteCarloTreeNode\n",
    "\n",
    "        @ËµµÂÆáÊÅí\n",
    "        \"\"\"\n",
    "        if not self._untried_actions:\n",
    "            self._untried_actions = state.get_legal_action()\n",
    "\n",
    "        #action = self._untried_actions.pop()\n",
    "\n",
    "        action = random.choice(self._untried_actions)\n",
    "        self._untried_actions.remove(action)\n",
    "\n",
    "        self.child[action] = MonteCarloTreeNode(self)\n",
    "        return action, self.child[action]\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        check if a node is a leaf node\n",
    "\n",
    "        a node is leaf if it is not fully expanded, or exists untried action\n",
    "\n",
    "        @ËµµÂÆáÊÅí\n",
    "        \"\"\"\n",
    "        return (self._untried_actions is None) or len(self._untried_actions)\n",
    "\n",
    "    def best_child(self):\n",
    "        \"\"\"\n",
    "        return the best child of current node\n",
    "\n",
    "        best action is decided by utc funtion\n",
    "\n",
    "        @ÂßúÊòüÂì≤\n",
    "        \"\"\"\n",
    "        #tmp = [child.uct() for child in self.child.values()]\n",
    "        #print(tmp)\n",
    "        return max(self.child.items(), key = lambda child: child[1].uct())\n",
    "\n",
    "    def uct(self, c_puct=np.sqrt(2)):\n",
    "        \"\"\"\n",
    "        Compute the upper confidence bound of current node\n",
    "\n",
    "        Parameters:\n",
    "        --------\n",
    "        c: float\n",
    "            constant for the UCT function\n",
    "\n",
    "        @ÂßúÊòüÂì≤\n",
    "        \"\"\"\n",
    "        return (self.n_win-self.n_lose)/self.n_visit + c_puct * np.sqrt(np.log(self.parent.n_visit) / self.n_visit)\n",
    "\n",
    "    def backpropagate(self, reward):\n",
    "        \"\"\"\n",
    "        @ÂêïÂÖ∞ÂÖ∞\n",
    "        \"\"\"\n",
    "        self.n_win += (reward==1)\n",
    "        self.n_lose += (reward==-1)\n",
    "        self.n_visit +=1 \n",
    "\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate(-reward)\n",
    "\n",
    "    def is_leaf_alpha(self):\n",
    "        \"\"\"@ÈÇ±‰∏ñËà™\"\"\"\n",
    "        return len(self.child)== 0\n",
    "\n",
    "    def best_child_alpha(self):\n",
    "        \"\"\"@ÈÇ±‰∏ñËà™\"\"\"\n",
    "        return max(self.child.items(), key = lambda child: child[1].uct_alpha())\n",
    "\n",
    "    def uct_alpha(self, c_puct=5):\n",
    "        \"\"\"@ÈÇ±‰∏ñËà™\"\"\"\n",
    "        _Q = (self.n_win-self.n_lose)/self.n_visit if self.n_visit > 0 else 0\n",
    "        return _Q + \\\n",
    "            c_puct * self.prior_prob * np.sqrt(self.parent.n_visit)/(1 + self.n_visit)\n",
    "\n",
    "    def expand_alpha(self, action_probs):\n",
    "        \"\"\"@ÈÇ±‰∏ñËà™\"\"\"\n",
    "        for action, prob in action_probs:\n",
    "            if actionld:\n",
    "                self.child[action] = MonteCarl, output_info=FalseoTreeNode(self, prior_prob=prob)\n",
    "\n",
    "class MonteCarloTreeSearch:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iter=20000, max_time=None, output_info=False):\n",
    "        \"\"\"\n",
    "        initialize a Monte Carlo Tree Search Algorithm\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_iter: int\n",
    "            maximum number of iteration\n",
    "        max_time: int\n",
    "            maximum time of si\n",
    "        self.output_info = Truemulation\n",
    "          @ËµµÂÆáÊÅí\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.root = MonteCarloTreeNode(None)\n",
    "        self.max_time = max_time\n",
    "        \n",
    "        self.output_info=output_info\n",
    "\n",
    "    def update_with_action(self, action):\n",
    "        \"\"\"\n",
    "        move to the next node\n",
    "\n",
    "        Parameters:\n",
    "        --------\n",
    "        action: (int, int)\n",
    "            last action\n",
    "\n",
    "        @ÈÇìÂáØÊñπ\n",
    "        \"\"\"\n",
    "        if action in self.root.child:\n",
    "            self.root = self.root.child[action]\n",
    "        else:\n",
    "            tmp = MonteCarloTreeNode(self.root)\n",
    "            self.root.child[action] = tmp\n",
    "            self.root = tmp\n",
    "\n",
    "    def run(self, state):\n",
    "        \"\"\"\n",
    "        run MCTS algorithm on given state\n",
    "\n",
    "        @ÈÇìÂáØÊñπ\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        n_draw = 0\n",
    "        for _ in range(self.n_iter):\n",
    "            if self.max_time and (time.time() - start_time > self.max_time):\n",
    "                if self.output_info:\n",
    "                    print(f\"number of iteration: {_}\")\n",
    "                break\n",
    "\n",
    "            state_copy = copy.deepcopy(state)\n",
    "            # get the node to run the simulation\n",
    "            node = self.select_node(state_copy)\n",
    "            reward = self.simulate(state_copy)\n",
    "            node.backpropagate(reward)\n",
    "\n",
    "            n_draw += (reward==0)\n",
    "            \n",
    "        if self.output_info:\n",
    "            print(f\"number of draw: {n_draw}\")\n",
    "        \n",
    "    \n",
    "\n",
    "    def select_node(self, state):\n",
    "        \"\"\"\n",
    "        select a leaf node for the simulation\n",
    "\n",
    "        @ÂßúÊòüÂì≤\n",
    "        \"\"\"\n",
    "        current_node = self.root\n",
    "        while not current_node.is_leaf():\n",
    "            action, current_node = current_node.best_child()\n",
    "            state.take_action(action)\n",
    "\n",
    "        if state.is_game_over():\n",
    "            return current_node\n",
    "        else:\n",
    "            action, node = current_node.expand(state)\n",
    "            state.take_action(action)\n",
    "            return node\n",
    "\n",
    "    def simulate(self, state):\n",
    "        \"\"\"\n",
    "        run single simulation (from node to terminal)\n",
    "\n",
    "        @ÈÇìÂáØÊñπ\n",
    "        \"\"\"\n",
    "        player_id = 1 - state.current_player_id\n",
    "        while not state.is_game_over():\n",
    "            actions = state.get_legal_action()\n",
    "\n",
    "            # random \n",
    "            action = random.choice(actions)\n",
    "            state.take_action(action)\n",
    "\n",
    "        if state.winner is None:\n",
    "            #print(\"simulation: draw\")\n",
    "            return 0\n",
    "        else:\n",
    "            #print(f\"simulation: {state.winner} wins\")\n",
    "            return 1 if state.winner == player_id else -1\n",
    "\n",
    "    def best_action(self):\n",
    "        \"\"\"\n",
    "        Get the best action\n",
    "\n",
    "        best action is the action of most visited child node\n",
    "\n",
    "        @ÂêïÂÖ∞ÂÖ∞\n",
    "        \"\"\"\n",
    "        return max(self.root.child.items(), key=lambda child: child[1].n_visit)[0]\n",
    "\n",
    "\n",
    "    def run_alpha(self, state, policy_func):\n",
    "        \"\"\"@ÈÇ±‰∏ñËà™\"\"\"\n",
    "        self.policy_func = policy_func\n",
    "        start_time = time.time()\n",
    "        for _ in range(self.n_iter):\n",
    "            if self.max_time and (time.time() - start_time > self.max_time):\n",
    "                print(f\"number of playout: {_}\")\n",
    "                break\n",
    "            state_copy = copy.deepcopy(state)\n",
    "            self.playout(state_copy)\n",
    "\n",
    "    def playout(self, state):\n",
    "        \"\"\"\n",
    "        run a single playout for alphazero mcts\n",
    "        @ÈÇ±‰∏ñËà™\n",
    "        \"\"\"\n",
    "        node = self.root            \n",
    "        while not node.is_leaf_alpha():\n",
    "            action, node = node.best_child_alpha()\n",
    "            state.take_action(action)\n",
    "\n",
    "        if not state.is_game_over():\n",
    "            action_probs, reward = self.policy_func(state)\n",
    "            node.expand_alpha(action_probs)\n",
    "        else:\n",
    "            if state.winner is None:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = 1 if state.winner == (1-state.current_player_id) else -1\n",
    "\n",
    "        node.backpropagate(reward)\n",
    "\n",
    "    \n",
    "    def get_action_probability(self, temp=1):\n",
    "        \"\"\"\n",
    "        get the probability for each action of self.root\n",
    "    \n",
    "        Parameters:\n",
    "        --------\n",
    "        temp: float\n",
    "            temperature parameters\n",
    "        \"\"\"\n",
    "\n",
    "        action_visit = {child[0]:child[1].n_visit for child in self.root.child.items()}\n",
    "\n",
    "        sum_visit = sum([visit**(1/temp) for visit in action_visit.values()])\n",
    "\n",
    "        action_prob = action_visit.copy()\n",
    "        for action in action_visit:\n",
    "            action_prob[action] = (action_visit[action]**(1/temp) / sum_visit)\n",
    "\n",
    "        return action_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ychs414V-IVO"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nX8rx76L3H6y"
   },
   "source": [
    "## 2.3 Game Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bonxpbf4JrQE"
   },
   "source": [
    "### 2.3.1 Gomoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gFH0Bkxs0VZ4"
   },
   "outputs": [],
   "source": [
    "class Gomoku:\n",
    "\n",
    "    def __init__(self, size, player1, player2, gui=True):\n",
    "        \"\"\"\n",
    "        initilize gomuku game\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        size: int\n",
    "            size of the board\n",
    "        player1, player2: GomokuGamePlayer\n",
    "            the two players of the Gomoku game\n",
    "        gui: bool\n",
    "            True if you want a graphical user interface\n",
    "        @ÈÇ±‰∏ñËà™\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.gui = gui\n",
    "        self.state = GomokuGameState(self.size, (player1, player2), start_player=0)\n",
    "\n",
    "        if gui:\n",
    "            # pygame\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((640,640))\n",
    "            pygame.display.set_caption('Five-in-a-Row')\n",
    "            self._draw_board()\n",
    "            pygame.display.update()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        run the Gomoku game\n",
    "        @ÂêïÂÖ∞ÂÖ∞\n",
    "        \"\"\"\n",
    "        while not self.state.is_game_over():\n",
    "            player = self.state.get_current_player()\n",
    "            action = player.get_action(self.state)\n",
    "            self.state.take_action(action)\n",
    "\n",
    "            if self.gui:\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        return\n",
    "                self.render()\n",
    "\n",
    "        ## handling game end\n",
    "        if self.state.winner is not None:\n",
    "            message = f\"Player {self.state.winner} wins!\"\n",
    "        else:\n",
    "            message = \"Draw!\"\n",
    "\n",
    "        if self.gui:\n",
    "            font = pygame.font.Font('freesansbold.ttf', 32)\n",
    "            text = font.render(message, True, (255, 255, 255), (0, 0, 0))\n",
    "            text_rect = text.get_rect()\n",
    "            text_rect.center = (320,320)\n",
    "            while True:\n",
    "                self.screen.blit(text, text_rect)\n",
    "                pygame.display.update()\n",
    "                for event in pygame.event.get():\n",
    "                     if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                         pygame.quit()\n",
    "                         return\n",
    "        else:\n",
    "            print(message)\n",
    "\n",
    "\n",
    "    def collect_play_data(self):\n",
    "        \"\"\"\n",
    "        run and collect data\n",
    "        @ÈÇ±‰∏ñËà™\n",
    "        \"\"\"\n",
    "        boards = []\n",
    "        players = []\n",
    "        probs = []\n",
    "        z = []\n",
    "        while not self.state.is_game_over():\n",
    "            player = self.state.get_current_player()\n",
    "            action, action_prob = player.get_action(self.state, return_prob=True)\n",
    "\n",
    "            ## store data\n",
    "            boards.append(self.state.get_board_under_current_player())\n",
    "            players.append(self.state.current_player_id)\n",
    "\n",
    "            action_id = [ac[0]*self.size+ac[1] for ac in action_prob.keys()]\n",
    "            prob = np.zeros(self.size*self.size)\n",
    "            prob[action_id] = list(action_prob.values())\n",
    "            probs.append(prob)\n",
    "\n",
    "            self.state.take_action(action)\n",
    "\n",
    "            if self.gui:\n",
    "                self.render()\n",
    "\n",
    "        z = np.zeros(len(players))\n",
    "        if self.state.winner is not None:\n",
    "            z[np.array(players) == self.state.winner] = 1\n",
    "            z[np.array(players) != self.state.winner] = -1\n",
    "\n",
    "        return zip(boards, probs, z)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Draw the updated game with lines and stones using function draw_board and draw_stone\n",
    "        input:\n",
    "            screen: game window, onto which the stones are drawn\n",
    "            mat: 2D matrix representing the game state\n",
    "        output:\n",
    "            none\n",
    "        \"\"\"\n",
    "        self._draw_board()\n",
    "        self._draw_stone()\n",
    "        pygame.display.update()\n",
    "\n",
    "    def _draw_board(self):    \n",
    "        \"\"\"\n",
    "        This function draws the board with lines.\n",
    "        input: game windows\n",
    "        output: none\n",
    "        \"\"\"\n",
    "        d=int(560/(self.size-1))\n",
    "        black_color = [0, 0, 0]\n",
    "        #board_color = [ 241, 196, 15]\n",
    "        board_color = [ 238,216,174]\n",
    "        self.screen.fill(board_color)\n",
    "        pygame.draw.circle(self.screen, black_color,(120,120), 6)\n",
    "        pygame.draw.circle(self.screen, black_color,(520,520), 6)\n",
    "        pygame.draw.circle(self.screen, black_color,(520,120), 6)\n",
    "        pygame.draw.circle(self.screen, black_color,(120,520), 6)\n",
    "        for h in range(0, self.size):\n",
    "            pygame.draw.line(self.screen, black_color,[40, h * d+40], [600, 40+h * d], 1)\n",
    "            pygame.draw.line(self.screen, black_color, [40+d*h, 40], [40+d*h, 600], 1)\n",
    "\n",
    "    def _draw_stone(self):\n",
    "        \"\"\"\n",
    "        This functions draws the stones according to the mat. It draws a black circle for matrix element 1(human),\n",
    "        it draws a white circle for matrix element -1 (computer)\n",
    "        input:\n",
    "            screen: game window, onto which the stones are drawn\n",
    "            mat: 2D matrix representing the game state\n",
    "        output:\n",
    "            none\n",
    "        \"\"\"\n",
    "        black_color = [0, 0, 0]\n",
    "        white_color = [255, 255, 255]\n",
    "\n",
    "        board = self.state.get_board()\n",
    "    \n",
    "        d=int(560/(self.size-1))\n",
    "        for i in range(board.shape[0]):\n",
    "            for j in range(board.shape[1]):\n",
    "                if board[i,j]==1:\n",
    "                    pos = [40+d * j, 40+d* i ]\n",
    "                    pygame.draw.circle(self.screen, black_color, pos, 18,0)\n",
    "                elif board[i,j]==-1:\n",
    "                    pos = [40+d* j , 40+d * i]\n",
    "                    pygame.draw.circle(self.screen, white_color, pos, 18,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nljBnsN6Ju5U"
   },
   "source": [
    "### 2.3.2 GomokuGamePlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CMpN09UmI0yS"
   },
   "outputs": [],
   "source": [
    "class GomokuGamePlayer(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for Gomoku game\n",
    "    @ÈÇ±‰∏ñËà™\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        get the player's action for current state\n",
    "\n",
    "        Parameters\n",
    "        --------\n",
    "        sate: GomokuGameState\n",
    "            current state of the game\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        action: (int, int)\n",
    "            location to move\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class MCTSPlayer(GomokuGamePlayer):\n",
    "    def __init__(self, n_iter=20000, max_time=None,output_info=False):\n",
    "        super().__init__()\n",
    "        self.mcts = MonteCarloTreeSearch(n_iter=n_iter, max_time=max_time,output_info = output_info)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Return an action for the current state\n",
    "        @ ÈÇìÂáØÊñπ\n",
    "        \"\"\"\n",
    "        # random action\n",
    "        # actions = state.get_legal_action()\n",
    "        # return tuple(random.choice(actions))\n",
    "\n",
    "        self.mcts.update_with_action(state.last_action)\n",
    "\n",
    "        self.mcts.run(state)\n",
    "        action = self.mcts.best_action()\n",
    "\n",
    "        self.mcts.update_with_action(action)\n",
    "\n",
    "        return action\n",
    "\n",
    "class HumanPlayer(GomokuGamePlayer):\n",
    "    \"\"\"Human player\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_action(self, state):\n",
    "     \n",
    "        while True:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type==pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit(0)\n",
    "                if event.type==pygame.MOUSEBUTTONDOWN:\n",
    "                    (x,y)=event.pos\n",
    "                    d=int(560/(state.size-1))\n",
    "                    row = round((y - 40) / d)\n",
    "                    col = round((x - 40) / d)\n",
    "\n",
    "                    action = (row, col)\n",
    "\n",
    "                    if state.is_legal_action(action):\n",
    "                        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHOg_eafJy2q"
   },
   "source": [
    "### 2.3.3 GomokuGameState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ibsofgg2ijdN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class GomokuGameState:\n",
    "    \"\"\"\n",
    "    game state for gomoku\n",
    "    \"\"\"\n",
    "    def __init__(self, size, players, start_player=0):\n",
    "        \"\"\"\n",
    "        initialize a Gomoku game state\n",
    "\n",
    "        Parameters:\n",
    "        --------\n",
    "        size: int\n",
    "            size of the board\n",
    "        players: (GomokuGamePlayer, GomokuGamePlayer)\n",
    "            two players of the Gomoku game\n",
    "        @ÈÇ±‰∏ñËà™\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.board = np.zeros((4, self.size, self.size), dtype=int)\n",
    "        self.players = players\n",
    "        self.current_player_id = start_player\n",
    "        self.last_action = None\n",
    "        self.winner = None\n",
    "        self.last_last_action = None\n",
    "\n",
    "        self._legal_actions = list(range(size*size)) # store row-wise as integers\n",
    "\n",
    "    def __deepcopy__(self, memodict={}):\n",
    "        \"\"\"\n",
    "        custom deepcopy function, for performance improvement\n",
    "        @ÈÇ±‰∏ñËà™\n",
    "        \"\"\"\n",
    "        state_copy = GomokuGameState(self.size, self.players)\n",
    "\n",
    "        state_copy.current_player_id = self.current_player_id\n",
    "        state_copy.board = np.copy(self.board)\n",
    "        state_copy.last_action = self.last_action\n",
    "        state_copy.last_last_action = self.last_last_action\n",
    "        state_copy.winner = self.winner\n",
    "        state_copy._legal_actions = self._legal_actions.copy()\n",
    "        return state_copy\n",
    "\n",
    "    def take_action(self, action):\n",
    "        \"\"\"\n",
    "        take action for current state\n",
    "\n",
    "        Parameters:\n",
    "        action: (int, int)\n",
    "            the location to take place\n",
    "        @ÂêïÂÖ∞ÂÖ∞\n",
    "        \"\"\"\n",
    "        self.board[self.current_player_id, action[0], action[1]] = 1\n",
    "\n",
    "        # change player\n",
    "        self.current_player_id = 1 - self.current_player_id\n",
    "        # update last action\n",
    "        self.last_last_action = self.last_action\n",
    "        self.last_action = action\n",
    "        # remove legal actions\n",
    "        self._legal_actions.remove(action[0]*self.size+action[1])\n",
    "\n",
    "    def get_current_player(self):\n",
    "        \"\"\"\n",
    "        return current player\n",
    "        @ÂêïÂÖ∞ÂÖ∞\n",
    "        \"\"\"\n",
    "        return self.players[self.current_player_id]\n",
    "\n",
    "    def is_game_over(self):\n",
    "        \"\"\"\n",
    "        check if the game is over\n",
    "        @ÈÇ±‰∏ñËà™\n",
    "        \"\"\"\n",
    "        if len(self._legal_actions) == 0:\n",
    "            return True\n",
    "        if self.last_action is None:\n",
    "            return False\n",
    "\n",
    "        last_player_id = 1 - self.current_player_id\n",
    "        board = self.board[last_player_id]\n",
    "\n",
    "        i, j = self.last_action\n",
    "\n",
    "        # check column\n",
    "        start, end = max(i-4, 0), min(i, self.size-1-4)\n",
    "        for k in range(start, end+1):\n",
    "            # if board[k:k+5, j].all():\n",
    "            if board[k,j] and board[k+1,j] and board[k+2,j] and board[k+3,j] and board[k+4,j]:\n",
    "                self.winner = last_player_id\n",
    "                return True  \n",
    "\n",
    "        # check rows\n",
    "        start, end = max(j-4,0), min(j, self.size-1-4)\n",
    "        for k in range(start, end+1):\n",
    "            # if board[i, k:k+5].all():\n",
    "            if board[i,k] and board[i,k+1] and board[i,k+2] and board[i,k+3] and board[i,k+4]:\n",
    "                self.winner = last_player_id\n",
    "                return True\n",
    "\n",
    "        # check diagonal\n",
    "        if i <= j:\n",
    "            offset = j-i\n",
    "            start, end = max(i-4, 0), min(i, self.size-1-4-offset)\n",
    "            for k in range(start, end+1):\n",
    "                # if board[range(k,k+5), range(k+(j-i),k+(j-i)+5)].all():\n",
    "                if board[k,k+offset] and board[k+1,k+1+offset] and board[k+2,k+2+offset] and \\\n",
    "                    board[k+3,k+3+offset] and  board[k+4, k+4+offset]:\n",
    "                    self.winner = last_player_id\n",
    "                    return True\n",
    "        if i > j:\n",
    "            offset = (i-j)\n",
    "            start, end = max(j-4, 0), min(j, self.size-1-4-offset)\n",
    "            for k in range(start, end+1):\n",
    "                # if board[range(k+(i-j),k+(i-j)+5), range(k,k+5)].all():\n",
    "                if board[k+offset,k] and board[k+1+offset,k+1] and board[k+2+offset,k+2] and \\\n",
    "                    board[k+3+offset,k+3] and board[k+4+offset,k+4]:\n",
    "                    self.winner = last_player_id\n",
    "                    return True\n",
    "\n",
    "        # check reverse diagonal\n",
    "        if i+j+1>=5 and i + j + 1 <= self.size: # i+j+1 is the size of the reverse diagonal\n",
    "            start, end = max(i-4,0), min(i, i+j-4)\n",
    "            for k in range(start, end+1):\n",
    "                if board[k,i+j-k] and board[k+1,i+j-k-1] and board[k+2,i+j-k-2] and board[k+3,i+j-k-3] \\\n",
    "                    and board[k+4,i+j-k-4]:\n",
    "                    self.winner = last_player_id\n",
    "                    return True\n",
    "        elif (i + j + 1 > self.size) and (2*self.size-i-j-1 >= 5): # size: self.size-(i+j+1-self.size)\n",
    "            start, end = max(i-4, i+j+1-self.size), min(i+4, max(0,self.size-5))\n",
    "            for k in range(start, end+1):\n",
    "                if board[k,i+j-k] and board[k+1,i+j-k-1] and board[k+2,i+j-k-2] and board[k+3,i+j-k-3] \\\n",
    "                    and board[k+4,i+j-k-4]:\n",
    "                    self.winner = last_player_id\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def get_legal_action(self):\n",
    "        \"\"\"\n",
    "        get all legal actions under current state\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        actions: list of (int, int)\n",
    "            list of legal actions\n",
    "        \n",
    "        @ÂßúÊòüÂì≤\n",
    "        \"\"\"\n",
    "        # convert integer to (row, col)\n",
    "        return [(t//self.size, t%self.size) for t in self._legal_actions]\n",
    "\n",
    "    def is_legal_action(self, action):\n",
    "        \"\"\"\n",
    "        check if an action is legal\n",
    "\n",
    "        Parameters:\n",
    "        -------\n",
    "        action: (int, int)\n",
    "        \n",
    "        @ÂßúÊòüÂì≤\n",
    "        \"\"\"\n",
    "        return action in self.get_legal_action()\n",
    "\n",
    "    def get_board(self):\n",
    "        \"\"\"\n",
    "        return a 2d array form of board, 1 for player 0 and -1 for player 1\n",
    "        @ËµµÂÆáÊÅí\n",
    "        \"\"\"\n",
    "        board = np.zeros((self.size, self.size))\n",
    "        board[self.board[0]==1] = 1\n",
    "        board[self.board[1]==1] = -1\n",
    "        return board\n",
    "\n",
    "    def get_board_under_current_player(self):\n",
    "        \"\"\"\n",
    "        get board data under current player\n",
    "        @ËµµÂÆáÊÅí\n",
    "        \"\"\"\n",
    "        a, b = self.current_player_id, 1 - self.current_player_id\n",
    "        board_cp = np.zeros((4,self.size,self.size))\n",
    "        board_cp[:2,:,:] = self.board[[a,b],:,:].copy()\n",
    "        board_cp[2, self.last_action[0], self.last_action[1]] = 1\n",
    "        board_cp[3] = (self.current_player_id == 0)\n",
    "        return board_cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbLBPQP5J9-S"
   },
   "source": [
    "## 2.4 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fj7GNmBFLcd0"
   },
   "outputs": [],
   "source": [
    "class MCTSPlayerAlpha(GomokuGamePlayer):\n",
    "    \"\"\"\n",
    "    A player with alphazero version of MCTS\n",
    "    \"\"\"\n",
    "    def __init__(self, policy_func, n_iter=20000,max_time=None):\n",
    "        self.policy_func = policy_func\n",
    "        self.mcts = MonteCarloTreeSearch(n_iter=n_iter,max_time=max_time)\n",
    "\n",
    "    def get_action(self, state, return_prob=False):\n",
    "        \"\"\"\n",
    "        Return an action for the current state\n",
    "        @ËµµÂÆáÊÅí\n",
    "        \"\"\"\n",
    "\n",
    "        self.mcts.update_with_action(state.last_action)\n",
    "        self.mcts.run_alpha(state, self.policy_func)\n",
    "\n",
    "        action_prob = self.mcts.get_action_probability()\n",
    "        action = max(action_prob.items(), key = lambda x: x[1])[0]\n",
    "        self.mcts.update_with_action(action)\n",
    "\n",
    "        if return_prob:\n",
    "            return action, action_prob\n",
    "        else:\n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyoAUz-PLUVV"
   },
   "source": [
    "run the code in appendix first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecM3OEpiLKSw"
   },
   "outputs": [],
   "source": [
    "net_param = pickle.load(open('zyh_8_8_5.model', 'rb', ), encoding='bytes')\n",
    "policy_value_fn = PolicyValueNetNumpy(8, 8, net_param).policy_value_fn\n",
    "\n",
    "gomoku = Gomoku(8, HumanPlayer(), MCTSPlayerAlpha(policy_value_fn,max_time=10))\n",
    "gomoku.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oz_vhoyrLmE6"
   },
   "outputs": [],
   "source": [
    "gomoku = Gomoku(8, MCTSPlayer(max_time=10), MCTSPlayerAlpha(policy_value_fn,max_time=10))\n",
    "gomoku.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVgTm4ovLDPp"
   },
   "source": [
    "![](https://chiuchiu.oss-cn-shenzhen.aliyuncs.com/img/20201220160247.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvQYAg-bBHF5"
   },
   "source": [
    "# 3. Individual Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WPJFi4UBK7y"
   },
   "source": [
    "## 3.1 Qiu Shihang\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE-FO4HZBKoN"
   },
   "source": [
    "In this project, I was mainly reponsible for\n",
    "\n",
    "- The design of the framework of our five-in-a-row games. Since this is a group project, it is important to break down the development of the games and algorithms into relative independent components.\n",
    "- Performance improvement, including code refactoring and the application of reinforcement learning.\n",
    "\n",
    "### 3.1.1 Class design\n",
    "\n",
    "The most basic component is `GomokuGameState` , it store the information of our current game state, including\n",
    "\n",
    "- `self.size`: size of the board\n",
    "- `self.board`: the board matrix of the game. Insteading of using a single `size*size` matrix to store the board, I use 2 `size*size` indicator(0-1) matrix for each players . This will simplify the checking of the win condition and is also a must in the training of reinforcement learning.\n",
    "- `self._untried_action`: all untried actions under current state, \n",
    "- others: players, current player, last action ...\n",
    "\n",
    "it also includes some methods to take action, to check whether the current state is game over or not, to get current player and so on.\n",
    "\n",
    "The second component is `GomokuGamePlayer`, I first use an abstract class to define that an object is a player if it could return an action given the current state. \n",
    "\n",
    "```python\n",
    "class GomokuGamePlayer(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def get_action(self, state):\n",
    "        # return action\n",
    "```\n",
    "\n",
    "Based on this, we can define a `HumanPlayer`, a `MCTSPlayer` using pure MCTS and a `MCTSPlayerAlpha` using policy neural network, we can also define game players with other strategies. With the help of this two class, the game procudure could be simplified as\n",
    "\n",
    "```python\n",
    "while not self.state.is_game_over():\n",
    "    player = self.state.get_current_player()\n",
    "    action = player.get_action(self.state)\n",
    "    self.state.take_action(action)\n",
    "```\n",
    "\n",
    "then the other parts of the game were move to a `Gomoku` class to handle the graphical user interface of the game.\n",
    "\n",
    "For the Monte Carlo Tree Search algorithms, there are three classes\n",
    "\n",
    "- `MonteCarloTreeSearch`: handling the tree search algorithm, we need to select a leaf node (`select_node`), we need to run the simulation on the leaf node (`simulation`) and we also need to find the best action after all simulation is done (`best_action`)\n",
    "- `MonteCarloTreeNode`: this is the tree structure that keep track of the whole Monte Carlo Tree, it should be able to determine whether a node is a leaf node (`is_leaf`), if so, it should `expand` and return the expaned child node. In each step of the simulation, we need to find out the `best_child` node and when the simulation is done, the reward should be `backpropagate`\n",
    "- `MCTSPlayer`: its `get_action` function define how to utilize the Monte Carlo Tree Search to find the best action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ah1q4ZvmiRUK"
   },
   "source": [
    "### 3.1.2 Performance Improvement\n",
    "\n",
    "In the first run of our program, the MCTS can only run for hundres of iteration in each step (in 10s limit), so I use python profiling tool to check the running time of each function and locate the major bottleneck `copy.deepcopy`.\n",
    "\n",
    "![image-20201220220526545](https://chiuchiu.oss-cn-shenzhen.aliyuncs.com/img/20201220220526.png)\n",
    "\n",
    "Since we use a single `GomokuGameState` object throughout the game, we need to make a deep copy of the object before we run the simulation of MCTS. Each `GomokuGameState` object will store the players and hence the Monte Carlo tree, so I write a custom `__deepcopy__` function in `GomokuGameState` to copy the minimum information we need to run the simulation. This change brings up the number of iteration up to thousands.\n",
    "\n",
    "The second observation of the bottleneck is the `is_game_over` function. \n",
    "\n",
    "![image-20201220221218399](https://chiuchiu.oss-cn-shenzhen.aliyuncs.com/img/20201220221218.png)\n",
    "\n",
    "Originally, I use `numpy.any()` to handle the check of the win condition (method 1), and then I found it much faster to use method 2 as the win condition. This may benefit from the \"shortcut\" properties of the `and` statement. The check of rows and columns are easy while the check of diagonal and reverse diagonal are much more complicated. We need to find the start and end location for checking.\n",
    "\n",
    "```python\n",
    "# check rows\n",
    "for k in range(start, end+1):\n",
    "    # method 1\n",
    "    if board[k:k+5,j].all():\n",
    "        ...\n",
    "        return True\n",
    "    # method 2\n",
    "    if board[k,j] and board[k+1,j] and board[k+2,j] and board[k+3,j] and board[k+4,j]:\n",
    "        ...\n",
    "        return True\n",
    "```\n",
    "\n",
    "After this change, MCTS can now runs up to ten thousands of iteration in 10s limit, which make the program start to behave intelligently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZBQH39NB9ea"
   },
   "source": [
    "## 3.2 Lyulanlan###\n",
    "\n",
    "\n",
    "### 3.2.1 My understanding\n",
    "\n",
    "[1] Selection \n",
    "\n",
    "In the selection phase, it is necessary to start from the root node, which is the decision-making situation R, to select a node N that needs to be expanded most urgently, and situation R is the first node to be checked in each iteration;\n",
    "\n",
    "For the situation being checked, he may have three possibilities:\n",
    "\n",
    "1. All feasible actions of the node have been expanded, then we will use the UCT formula to calculate the UCT value of all child nodes of the node, and find the child node with the largest value to continue checking. Iterate down repeatedly.\n",
    "\n",
    "2. This node has feasible actions that have not been expanded, then we think that this node is the target node N of this iteration, and find the action A whose N has not been expanded, and continue to expand.\n",
    "\n",
    "3. This node game has ended, and it is directly backpropagated.\n",
    "\n",
    "\n",
    "The number of visits of each checked node will increase automatically at this stage.\n",
    "\n",
    "After repeated iterations, we will find a node at the bottom of the search tree to continue the next steps.\n",
    "\n",
    "[2] Expansion\n",
    "\n",
    "At the end of the selection phase, we found an optimally selected node N and his unexpanded action A. Create a new node Nn as a new child node of N in the search tree. Node N becomes Nn after performing action A.\n",
    "\n",
    "[3] Simulation\n",
    "\n",
    "In order for Nn to get an initial score. We start with $N_n$ and let the game proceed randomly until we get a game ending, which will be the initial score of Nn. Generally use victory/loss as a score, with only 1 or 0.\n",
    "\n",
    "[4] Backpropagation\n",
    "\n",
    "After the simulation of $N_n$ is over, its parent node N and all nodes on the path from the root node to N will add their own cumulative scores based on the results of this simulation. If a game ending is directly found in the selection, the score is updated according to the ending.\n",
    "\n",
    "Backpropagation is to traverse from the leaf node (the node where the simulation starts) back to the root node. The simulation results are passed to the root node, and for each node on the back propagation path, specific statistics are calculated or updated. Backpropagation ensures that the statistics of each node will reflect the results of all simulations starting from its descendant nodes (because the simulation results are passed to the root node of the game tree).\n",
    "![](https://chiuchiu.oss-cn-shenzhen.aliyuncs.com/img/20201220202635.jpg)\n",
    "\n",
    "The purpose of backpropagation simulation results is to update the total simulation return and the total number of visits of all nodes on the backpropagation path (including the node where the simulation started).\n",
    "‚Äî‚ÄîSimulation total return, an attribute of the node, in simple terms, is the summary of all simulation results through the node.\n",
    "-The total number of visits, another attribute of the node, indicates how many times the backpropagation path passes through this node (and how many times it contributes to the total return of the simulation).\n",
    "\n",
    "###3.2.1 My work\n",
    "I take charge of part of Main function and Action function and the Back propagate function.\n",
    "\n",
    "According to the algorithm, this Monte Carlo tree is more inclined to search for child nodes with good performance. Therefore, the best child for the root node is always the child node with the most frequent visits, so we can choose the next action to be expressed as , Directly query the number of visited by all child nodes, and then find the maximum value. The function best_action() and take_action() performed this job.\n",
    " \n",
    "The Back propagate function is used to return the simulation information of the child node. When searching for a Monte Carlo tree, we will traverse the entire tree forward from the root node until we select (or expand) a suitable leaf node. Then rollout the selected leaf node, and you will get a simulation result. This result is used to update the statistics of the current node, that is, if the winner of simulation is the same as the prescription of the current node, the number of wins will be accumulated once, otherwise it will not be accumulated. And every simulation, regardless of the victory or defeat, the number of traversals will be added once. These two updated values ‚Äã‚Äãcan be combined with the total number of traverses to obtain UCT, which is the basis for obtaining the best child node. After updating the current node, this value will start to trace the entire tree from the current node, and update the ancestor nodes of this node one by one according to the same rules until the root node is traced back.\n",
    "\n",
    "###3.2.1 What I learn\n",
    "1. Thorough understanding of Monte Carlo tree search, Monte Carlo tree search can be regarded as a general technology for decision-making in a perfect information game scenario. So its application field is not limited to the game field. There can be other attempts in the future.\n",
    "\n",
    "2. Learn a lot of new programming thinking, programming methods and programming skills from teammates.\n",
    "\n",
    "2. The teamwork ability is improved, which is helpful for future study and work.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfgTT46cQH_t"
   },
   "source": [
    "## 3.3 Deng Kaifang\n",
    "My main work in this project:\n",
    "\n",
    "1.Based on the understanding of the whole MCTS algorithm and the frame of the project code, realize the functions in details.\n",
    "\n",
    "2.Responsible for main design of the class `MenteCarloTreeSearch` and integrate other functions together then develope the main process of the MCTS algorithm.\n",
    "\n",
    "3.Design the class MCTS player based on the abstract class `GomokuGamePlayer`\n",
    "\n",
    "4.Preliminary realize the MCTS algorithm and combine it into the project framework.\n",
    "\n",
    "\n",
    "### 3.3.1 Understanding of the MenteCarloTreeSearch\n",
    "\n",
    "The MCTS algorithm is a heuristic search algorithm.From my perspective, it can carry out a large number of random simulations to find the best move for the current state in this game. It follows four steps: Selection, Expansion, Simulation and Back-propagation and the search tree would perform the process once in an iteration. The search tree would repeat the process, in other words, do many iterations to get the statistics information of each possible move so as to find the best move.\n",
    "\n",
    "Of course,the original MCTS has its limitation, that if each simulation just take action randomly, it's very hard to converge to a good result. So naturally people use UCT algorithm to choose and expand the child node in the real process. I think it is very smart to use not only the winning probability of each node, but also focus on the node who has less simulate times in the long time scale simulation.We can choose the node by simulated winning probability, but UCT algorithm give us a way to explore new way to win, which i think it's very important in this search tree.\n",
    "\n",
    "As a summary, i think Mente Carlo Tree Search is a very solid algorithm in the self learning AI algorithm, which can be combined with the Reinforcement Learning and other method to get a better AI.\n",
    "\n",
    "### 3.3.2 My work on this project\n",
    "\n",
    "Firstly, in the class `MonteCarloTreeSearch`, i response to develope the function `update_with_action()`, `run()`, `simulate()`.\n",
    "\n",
    "`update_with_action()` is using to update the current node after a new move. Perform this function we can change the current node to the new node according to the action we take.If it is the first move of a new game, the state will be the root node.\n",
    "\n",
    "`simulate` is using to do once simulation in a chosen node. While the game is not over,it will continuously choose possible move randomly and take action. At the end, return the winner and the game condition, win, lose or draw.\n",
    "\n",
    "`run()` is main working process in the class MonteCarloTreeSearch,which integrate other functions and run the whole MCTS simulation process.\n",
    "\n",
    "```python\n",
    " def run(self, state):\n",
    "        \"\"\"\n",
    "        run MCTS algorithm on given state\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        for _ in range(self.n_iter):\n",
    "            if self.max_time and (time.time() - start_time > self.max_time):\n",
    "                print(f\"number of iteration: {_}\")\n",
    "                break\n",
    "\n",
    "            state_copy = copy.deepcopy(state)\n",
    "            # get the node to run the simulation\n",
    "            node = self.select_node(state_copy)\n",
    "            reward = self.simulate(state_copy)\n",
    "            node.backpropagate(reward)\n",
    "```\n",
    "\n",
    "It combines the main function of this class to finish a number of simulation and node update. In details, `run()` will run simulations until the max_time has been reached.In each iteration, it calls funciton `select_node()` to select the node which will be performed simulations,function `simulate()` to do once simulation in the chosen node as well as return the reward value and function `backpropagate` to back propagate all the nodes in the simulation path.\n",
    "\n",
    "Secondly, in the class MCTSPlayer, i response to develop the funcion `get_action`.The class is based on the class `GomokuGamePlayer`,which is an abstract class of a game player.\n",
    "\n",
    "`get_action` is using to get the last action and the state,in other words, the state after the other player take moving, and choose a best move in the current state, and simultaneously update the current node to the next node.\n",
    "\n",
    "### 3.3.3 The thoughts from this report\n",
    "\n",
    "It is a very good chance for me to work with my partners to complete this intersting project together,during which i got a better knowing on my nice classmates. I am very happy and enrich myself.\n",
    "\n",
    "In order to better understand the MenteCarloSearchTree, i try to realize the whole tree by myself, and thus i get a fully understand about the MCTS algorithm by realizing it as well as take many discussions with my partners.\n",
    "\n",
    "Although it is only one kind of way to design a game AI, but it enrich me much more interst in exploring how this algorithm working in many areas.I also learn how to optimize my code and algorithm step by step.And the experience of how to perform a team development of a project is very important. And i can do much better in the next challenge work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewDiV-AVTGAz"
   },
   "source": [
    "## 3.4 Jiang Xingzhe\n",
    "\n",
    "#### 3.4.1 Understanding of MCTS\n",
    "\n",
    "Monte Carlo tree search(MCTS) is a very huge statistical algorithm designed to abstractly simulate some specific processes and get results which is hard to directly obtain by general mathematical formulas. In this project, we uses a simplified MCTS Model designed for chess, divided into four steps:\n",
    "\n",
    "1. Selection\n",
    "\n",
    "   Get the best leaf node which is worth to traverse and simulate.\n",
    "\n",
    "2. Expansion\n",
    "\n",
    "   Find all child nodes of the selected nodes .\n",
    "\n",
    "3. Simulation\n",
    "\n",
    "   Simulate the node's future situation until game over, record it's win value and visited time.\n",
    "\n",
    "4. Backpropagation\n",
    "\n",
    "   Update parent nodes' win value and visited time until the root node. Back to the step 1.\n",
    "\n",
    "Generally, MCTS is a self-upgrading model that can become more and more intelligent by constantly simulating the game  and iterating itself to find out the best next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTXLxsc6WI42"
   },
   "source": [
    "## 3.5 Zhao Yuheng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Run on 4*4 board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether the MCTS sucessfully runs in our gomoku game, we runs it on the $4\\times4$ board. The results should all draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "number of draw: 2000\n",
      "Draw!\n"
     ]
    }
   ],
   "source": [
    "gomoku = Gomoku(4, MCTSPlayer(n_iter=2000,output_info=True), MCTSPlayer(n_iter=2000,output_info=True), gui=False)\n",
    "gomoku.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Runs on 6*6 board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On $6\\times 6$ board, the computer is smart enough to block our move. Our group members have played several times with computer on $6\\times 6$ board, most of the time it will be a draw, if we are not care enough, the computer may even wins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gomoku = Gomoku(6, HumanPlayer(), MCTSPlayer(max_time=10), gui=True)\n",
    "gomoku.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://chiuchiu.oss-cn-shenzhen.aliyuncs.com/img/20201220232058.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Runs on 8*8 board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On $8\\times 8$ board, the computer may block your move, it may also failed to take the right place as its shown in the below right graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gomoku = Gomoku(8, HumanPlayer(), MCTSPlayer(max_time=10), gui=True)\n",
    "gomoku.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://chiuchiu.oss-cn-shenzhen.aliyuncs.com/img/20201220232655.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSoI3-AeLQ2i"
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUCdUyrALQmE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    probs = np.exp(x - np.max(x))\n",
    "    probs /= np.sum(probs)\n",
    "    return probs\n",
    "\n",
    "def relu(X):\n",
    "    out = np.maximum(X, 0)\n",
    "    return out\n",
    "\n",
    "def conv_forward(X, W, b, stride=1, padding=1):\n",
    "    n_filters, d_filter, h_filter, w_filter = W.shape\n",
    "    # theano conv2d flips the filters (rotate 180 degree) first\n",
    "    # while doing the calculation\n",
    "    W = W[:, :, ::-1, ::-1]\n",
    "    n_x, d_x, h_x, w_x = X.shape\n",
    "    h_out = (h_x - h_filter + 2 * padding) / stride + 1\n",
    "    w_out = (w_x - w_filter + 2 * padding) / stride + 1\n",
    "    h_out, w_out = int(h_out), int(w_out)\n",
    "    X_col = im2col_indices(X, h_filter, w_filter,\n",
    "                           padding=padding, stride=stride)\n",
    "    W_col = W.reshape(n_filters, -1)\n",
    "    out = (np.dot(W_col, X_col).T + b).T\n",
    "    out = out.reshape(n_filters, h_out, w_out, n_x)\n",
    "    out = out.transpose(3, 0, 1, 2)\n",
    "    return out\n",
    "\n",
    "\n",
    "def fc_forward(X, W, b):\n",
    "    out = np.dot(X, W) + b\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_im2col_indices(x_shape, field_height,\n",
    "                       field_width, padding=1, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    N, C, H, W = x_shape\n",
    "    assert (H + 2 * padding - field_height) % stride == 0\n",
    "    assert (W + 2 * padding - field_height) % stride == 0\n",
    "    out_height = int((H + 2 * padding - field_height) / stride + 1)\n",
    "    out_width = int((W + 2 * padding - field_width) / stride + 1)\n",
    "\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "    return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height,\n",
    "                                 field_width, padding, stride)\n",
    "\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "class PolicyValueNetNumpy():\n",
    "    \"\"\"policy-value network in numpy \"\"\"\n",
    "\n",
    "    def __init__(self, board_width, board_height, net_params):\n",
    "        self.board_width = board_width\n",
    "        self.board_height = board_height\n",
    "        self.params = net_params\n",
    "\n",
    "        tmp = (np.array(np.arange(64)).reshape(8,8))\n",
    "        self.action_map =  dict(zip(tmp.flatten().tolist(), np.flipud(tmp).flatten().tolist()))\n",
    "\n",
    "    def policy_value_fn(self, state):\n",
    "        \"\"\"\n",
    "        input: board\n",
    "        output: a list of (action, probability) tuples for each available\n",
    "        action and the score of the board state\n",
    "        \"\"\"\n",
    "       \n",
    "        legal_positions = state._legal_actions\n",
    "        current_state = state.get_board_under_current_player()\n",
    "        \n",
    "        X = current_state.reshape(-1, 4, self.board_width, self.board_height)\n",
    "        # first 3 conv layers with ReLu nonlinearity\n",
    "        for i in [0, 2, 4]:\n",
    "            X = relu(conv_forward(X, self.params[i], self.params[i + 1]))\n",
    "        # policy head\n",
    "        X_p = relu(conv_forward(X, self.params[6], self.params[7], padding=0))\n",
    "        X_p = fc_forward(X_p.flatten(), self.params[8], self.params[9])\n",
    "        act_probs = softmax(X_p)\n",
    "        # value head\n",
    "        X_v = relu(conv_forward(X, self.params[10],\n",
    "                                self.params[11], padding=0))\n",
    "        X_v = relu(fc_forward(X_v.flatten(), self.params[12], self.params[13]))\n",
    "        value = np.tanh(fc_forward(X_v, self.params[14], self.params[15]))[0]\n",
    "        \n",
    "        actions = [(pos//8, pos%8) for pos in legal_positions]\n",
    "        act_probs = zip(actions, act_probs.flatten()[[self.action_map[pos] for pos in legal_positions]])\n",
    "\n",
    "        return act_probs, value"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
